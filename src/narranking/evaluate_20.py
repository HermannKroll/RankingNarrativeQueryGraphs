import json
import os
from collections import defaultdict
from typing import List

from narraplay.documentranking import evaluate
from narraplay.documentranking.benchmark import Benchmark
from narraplay.documentranking.config import EVAL_DIR, RESULT_DIR_FIRST_STAGE, RESULT_DIR
from narraplay.documentranking.evaluate import load_results, load_document_count_statistics, calculate_table_data
from narraplay.documentranking.run_config import CONCEPT_STRATEGIES, FIRST_STAGE_NAMES, SKIPPED_TOPICS, \
    EVALUATION_SKIP_BAD_TOPICS, FIRST_STAGE_TO_PRINT_NAME

METHODS_TO_REPORT_IN_PAPER = {
    "EqualDocumentRanker": "",
    "WeightedDocumentRanker_min-0": "GraphRank",
    "BM25Text": "BM25",
    "BM25 Native": "BM25 Native"
}

RESULT_MEASURES = {
    'recall_1000': 'Recall@1000',

    'ndcg_cut_10': 'nDCG@10',
    'ndcg_cut_20': 'nDCG@20',
    'ndcg_cut_100': 'nDCG@100',

    'P_10': 'P@10',
    'P_20': 'P@20',
    'P_100': 'P@100',
}

BENCHMARK = Benchmark("trec-pm-2020-abstracts", "", ["PubMed"], load_from_file=True)


def generate_table(measures: dict, strategies: str, first_stages: List[str], benchmark: Benchmark,
                   min_docs_per_topic: int = None, min_recall: float = 0.0):
    score_rows_fs = defaultdict(defaultdict)
    measures = [(k, v) for k, v in measures.items()]

    empty_measures = {k: "-" for k, _ in measures}  # empty entries for table generation
    relevant_rankers = set(METHODS_TO_REPORT_IN_PAPER.keys())

    for first_stage in first_stages:
        for strategy in strategies:
            print("--" * 60)
            print(f"Generating score table for FirstStage {first_stage} "
                  f"with benchmark {benchmark.name} and strategy {strategy}")
            path = os.path.join(EVAL_DIR, f"{first_stage}_{strategy}_", f"{benchmark.name}")
            results = load_results(path)

            skipped_topics = SKIPPED_TOPICS[benchmark.name].copy()

            if EVALUATION_SKIP_BAD_TOPICS:
                # Load statistics concerning translation and components
                stats_dir = os.path.join(RESULT_DIR_FIRST_STAGE, 'statistics')
                stats_path = os.path.join(stats_dir, f'{benchmark.name}_{first_stage}_{strategy}.json')
                with open(stats_path, 'rt') as f:
                    stats = json.load(f)

                for topic_id in stats:
                    if 'skipped' in stats[topic_id]:
                        skipped_topics.append(topic_id)

            relevant_topics = {str(q.query_id) for q in benchmark.topics
                               if str(q.query_id) not in skipped_topics}
            print('==' * 60)
            print(f'Evaluation based on {len(relevant_topics)} topics')
            print('==' * 60)
            results = [(k, v) for k, v in sorted(results.items(), key=lambda x: x[0])]

            if min_docs_per_topic:
                path = os.path.join(RESULT_DIR, "statistics", f"{benchmark.name}_{first_stage}_{strategy}.json")
                doc_count = load_document_count_statistics(path)
                topics_with_less_docs = {q_id for q_id, doc_c in doc_count if doc_c < min_docs_per_topic}
                relevant_topics = relevant_topics.difference(topics_with_less_docs)
                print(f"Topics with less than {min_docs_per_topic} docs: {topics_with_less_docs}")
                print(f"{len(relevant_topics)} relevant topics: {relevant_topics}")

            if min_recall:
                topics_with_less_recall = set()
                for r_name, r_result in results:
                    for q, scores in r_result.items():
                        if scores['set_recall'] < min_recall:
                            topics_with_less_recall.add(q)

                relevant_topics = relevant_topics.difference(topics_with_less_recall)
                print(f"Topics with less than {min_recall} recall docs: {topics_with_less_recall}")
                print(f"{len(relevant_topics)} relevant topics: {relevant_topics}")

            score_rows, max_m = calculate_table_data(measures, results, relevant_topics, relevant_rankers)
            score_rows_fs[first_stage][strategy] = (score_rows, max_m)

    print('==' * 60)
    print("--" * 60)
    print("Creating table content")
    print("--" * 60)

    # create tabular LaTeX code
    rows = list()
    rows.append("%%%%% begin autogenerated %%%%%")
    rows.append(r"\begin{tabular}{lccccccc}")

    rows.append(r"\toprule")
    rows.append(" & ".join(["Ranking Method", *(str(m[1]) for m in measures)]) + r" \\")
    rows.append(r"\midrule")

    rows.append(r"Old System  & 0.24 & 0.30 & 0.30 & 0.28 & 0.34 & 0.31 & 0.16 \\")

    for first_stage in first_stages:
        rows.append(f"\n\\midrule  % {first_stage}\n")

        max_m = {k: 0.0 for k in RESULT_MEASURES}
        # merge max values over all strategies
        for strategy in strategies:
            score_rows, _ = score_rows_fs[first_stage][strategy]
            for m in max_m.keys():
                # ignore BM25 Native scores
                max_m[m] = max([max_m[m], *(sv[m] for k, sv in score_rows.items() if k != "BM25 Native")])

        for strategy in strategies:
            score_rows, _ = score_rows_fs[first_stage][strategy]

            for method, method_name in METHODS_TO_REPORT_IN_PAPER.items():
                if method_name == "BM25 Native":
                    continue

                if not method_name and "ontology" not in strategy:
                    method_name = rf"\textbf{{{FIRST_STAGE_TO_PRINT_NAME[first_stage]}}}"
                elif not method_name and "ontology" in strategy:
                    method_name = rf"\quad + Ontology"
                elif method_name and "ontology" in strategy:
                    method_name = rf"\quad + Ontology + {method_name}"
                else:
                    method_name = rf"\quad + {method_name}"

                scores = score_rows.get(method, empty_measures)
                row = f"{method_name} & "
                row += " & ".join(rf"\textbf{{{str(s)}}}" if max_m[m] == s else str(s)
                                  for m, s in scores.items())
                row += r" \\"
                rows.append(row)

    rows.append(r"\midrule")

    # handle BM25 Native separately
    score_rows, max_m = score_rows_fs[first_stages[1]][strategies[1]]
    scores = score_rows.get("BM25 Native", empty_measures)

    row = rf"\textbf{{Native BM25 (Baseline)}} & "
    row += " & ".join(rf"\textbf{{{str(s)}}}" if max_m[m] == s else str(s)
                      for m, s in scores.items())
    row += r" \\"
    rows.append(row)

    rows.append(r"\bottomrule")
    rows.append(r"\end{tabular}")
    rows.append("%%%%% end autogenerated %%%%%")

    print("\n".join(rows))
    print("--" * 60)


if __name__ == "__main__":
    evaluate.evaluate_runs(with_bm25_native=True, benchmarks=[BENCHMARK])
    generate_table(RESULT_MEASURES, CONCEPT_STRATEGIES, FIRST_STAGE_NAMES, BENCHMARK)
